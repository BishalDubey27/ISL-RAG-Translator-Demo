=================================================================
QUICK OPTIMIZATION STEPS - START HERE
Implement these 5 changes for immediate 50-70% latency reduction
=================================================================

## STEP 1: Optimize Frame Processing (2 minutes)

File: templates/live_sign_recognition.html

Find the capture loop and add:

```javascript
// Add these constants at the top
const FRAME_SKIP = 2;  // Process every 2nd frame
const TARGET_WIDTH = 640;
const TARGET_HEIGHT = 480;
const JPEG_QUALITY = 0.7;

let frameCounter = 0;

// In your capture function:
async function captureAndProcess() {
    frameCounter++;
    
    // Skip frames
    if (frameCounter % FRAME_SKIP !== 0) {
        return;
    }
    
    // Resize canvas
    canvas.width = TARGET_WIDTH;
    canvas.height = TARGET_HEIGHT;
    context.drawImage(video, 0, 0, TARGET_WIDTH, TARGET_HEIGHT);
    
    // Compress JPEG
    canvas.toBlob(async (blob) => {
        await sendToServer(blob);
    }, 'image/jpeg', JPEG_QUALITY);
}
```

Expected improvement: 40-50% faster

## STEP 2: Optimize MediaPipe Extractor (5 minutes)

File: sign_recognition/models/mediapipe_extractor.py

Add frame resizing in extract_frame method:

```python
def extract_frame(self, frame: np.ndarray):
    # Add this at the start of the method
    # Resize for faster processing
    if frame.shape[0] > 480 or frame.shape[1] > 640:
        frame = cv2.resize(frame, (640, 480))
    
    # ... rest of existing code
```

Expected improvement: 30-40% faster

## STEP 3: Enable GPU and FP16 (3 minutes)

File: sign_recognition/utils/inference.py

In the __init__ method, after loading model:

```python
# After: self.model.to(self.device)
# Add:

# Optimize for inference
self.model.eval()

# Use FP16 on GPU for 2x speed
if self.device.type == 'cuda':
    self.model = self.model.half()
    print("âœ“ Using FP16 (half precision) for faster inference")
```

In predict_video method:

```python
# After: features_tensor = torch.from_numpy(features).float().unsqueeze(0).to(self.device)
# Add:

# Convert to FP16 if using GPU
if self.device.type == 'cuda':
    features_tensor = features_tensor.half()
```

Expected improvement: 50% faster on GPU

## STEP 4: Add Prediction Smoothing (5 minutes)

File: unified_app.py

Add this class before the routes:

```python
from collections import deque, Counter

class PredictionSmoother:
    def __init__(self, buffer_size=5):
        self.buffer = deque(maxlen=buffer_size)
    
    def add(self, label, confidence):
        self.buffer.append((label, confidence))
    
    def get_smoothed(self):
        if len(self.buffer) < 3:
            return self.buffer[-1] if self.buffer else (None, 0.0)
        
        labels = [p[0] for p in self.buffer]
        most_common = Counter(labels).most_common(1)[0][0]
        
        confidences = [c for l, c in self.buffer if l == most_common]
        avg_conf = sum(confidences) / len(confidences)
        
        return (most_common, avg_conf)

# Create global instance
prediction_smoother = PredictionSmoother()
```

In your recognition endpoint, use it:

```python
# After getting prediction
prediction_smoother.add(predicted_text, confidence)
smoothed_text, smoothed_conf = prediction_smoother.get_smoothed()

# Return smoothed prediction
return jsonify({
    'recognized_text': smoothed_text,
    'confidence': smoothed_conf,
    'raw_text': predicted_text,  # Original
    'raw_confidence': confidence
})
```

Expected improvement: Better accuracy, smoother results

## STEP 5: Reduce Client Frame Rate (1 minute)

File: templates/live_sign_recognition.html

Find where you set up the camera/capture interval:

```javascript
// Change from 30 FPS to 15 FPS
const FPS = 15;  // Was 30
const INTERVAL = 1000 / FPS;

// Use this interval for capture
setInterval(captureAndProcess, INTERVAL);
```

Expected improvement: 50% less processing load

=================================================================
VERIFICATION
=================================================================

After implementing, test with:

1. Open browser console (F12)
2. Add timing code:

```javascript
const start = performance.now();
await sendToServer(blob);
const end = performance.now();
console.log(`Latency: ${(end - start).toFixed(0)}ms`);
```

3. Check Flask terminal for:
   - "Using FP16" message
   - Processing times

Target metrics:
- Before: 500-1000ms latency
- After: 200-400ms latency
- FPS: 10-15 (smooth enough for sign language)

=================================================================
TROUBLESHOOTING
=================================================================

### GPU not being used?
Check with:
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Device: {torch.cuda.get_device_name(0)}")
```

### Still slow?
1. Check frame resolution (should be 640x480)
2. Verify frame skip is working (console.log frameCounter)
3. Ensure JPEG quality is 0.7 (not 1.0)
4. Check if MediaPipe models are cached

### Accuracy dropped?
1. Reduce frame skip (use 1 instead of 2)
2. Increase smoothing buffer (use 7 instead of 5)
3. Increase JPEG quality (use 0.8 instead of 0.7)

=================================================================
EXPECTED RESULTS
=================================================================

Before optimization:
- Latency: 800-1200ms
- FPS: 5-8
- CPU: 80-100%
- GPU: 20-30%

After optimization:
- Latency: 200-400ms (60-70% improvement!)
- FPS: 12-18
- CPU: 40-60%
- GPU: 60-80%

=================================================================
